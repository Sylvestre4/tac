{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10600de3",
   "metadata": {},
   "source": [
    "  ## Clustering sur la decennie "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ccadc",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cac0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a198ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc833",
   "metadata": {},
   "source": [
    "## Choisir une décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7146b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1960'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087fb4d",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers de la décennie et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94beaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb344580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dc52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83098f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61feabd5",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89a94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8dfbb",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b704a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2bfd4",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f198a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37913ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e4fdc",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55585412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e585d27",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376caff",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96cf0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313becaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9cef5",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5b77d",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b7031e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebf8be",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7bae6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9689e88",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c727a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f775245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clusters = km_model.fit_predict(tfidf_vectors)\n",
    "\n",
    "# Obtenez les noms des features (mots) à partir du vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Définir le nombre de mots-clés à extraire par cluster\n",
    "num_keywords = 20\n",
    "\n",
    "print(f\"\\n--- Top {num_keywords} mots-clés par cluster (pour K={km_model.n_clusters}) ---\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Pour chaque cluster identifié par K-Means\n",
    "for i in range(km_model.n_clusters):\n",
    "    cluster_tfidf_vectors = tfidf_vectors[clusters == i]\n",
    "    if cluster_tfidf_vectors.shape[0] > 0:\n",
    "        cluster_tfidf_mean = cluster_tfidf_vectors.mean(axis=0) \n",
    "        top_indices = np.array(cluster_tfidf_mean).flatten().argsort()[::-1][:num_keywords]\n",
    "\n",
    "        # Récupérer les mots correspondants\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "\n",
    "        # Obtenir le nombre de documents dans ce cluster\n",
    "        num_docs_in_cluster = cluster_tfidf_vectors.shape[0]\n",
    "\n",
    "        print(f\"Cluster {i} (Documents: {num_docs_in_cluster}):\")\n",
    "        print(f\"  Mots-clés : {', '.join(top_words)}\")\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "        print(f\"Cluster {i} est vide.\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22877984",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adbc82",
   "metadata": {},
   "source": [
    "### Réduire les vecteurs à 2 dimensions à l'aide de l'algorithme PCA\n",
    "Cette étape est nécessaire afin de visualiser les documents dans un espace 2D\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b559cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebda383",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e5280",
   "metadata": {},
   "source": [
    "# Word Embeddings : le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1bc0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc95785",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b1c65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba91face",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d1c1a",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df4f72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dec5c",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629993d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537d074",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26578622",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122b2b3",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c149b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b342ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5354d",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af402c",
   "metadata": {},
   "source": [
    "Nous répétons l'opération en envoyant cette fois la liste de bigrams afin d'extraire les trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd6ff557",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ce2bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654f422",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "344cfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc95970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c21cc",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "42182d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Stopwords de base en français\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Stopwords personnalisés pour nettoyer les artefacts typographiques et mots peu informatifs\n",
    "custom_stopwords = {\n",
    "    '--', '>>', '<<', '...', '«', '»', '’', '``', \"''\", '.', ',', ';', ':', '!', '?',\n",
    "    'http', 'https', 'www', 'com', 'fr', 'org', 'là', 'ainsi', 'toutefois',\n",
    "    'article', 'chapitre', 'figure', 'exemple', 'etc', 'cela', 'celui', 'ceux',\n",
    "    'très', 'peu', 'dont', 'tandis', 'lorsque', 'entre', 'chaque', 'aucun'\n",
    "}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Ponctuation standard\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Nettoyage du corpus\n",
    "corpus_cleaned = []\n",
    "for doc in corpus:\n",
    "    cleaned_doc = []\n",
    "    for word in doc:\n",
    "        word = word.lower().strip()\n",
    "        if (\n",
    "            len(word) > 2 and               # Ignore mots trop courts\n",
    "            not word.isdigit() and         # Ignore chiffres\n",
    "            word not in stop_words and     # Ignore stopwords\n",
    "            word not in punctuation and    # Ignore ponctuation\n",
    "            re.match(\"^[a-zàâçéèêëîïôûùüÿñæœ-]+$\", word)  # Garde uniquement les mots alpha valides\n",
    "        ):\n",
    "            cleaned_doc.append(word)\n",
    "    if cleaned_doc:\n",
    "        corpus_cleaned.append(cleaned_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04deb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus_cleaned,\n",
    "    vector_size=32,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded5986",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177e74d",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb92a56",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184def3a",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"ministre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbd6da",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dc3130bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56965774"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfd97e",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec5b1d",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aecaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import collections\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"t-SNE et Matplotlib importés avec succès.\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib ou scikit-learn non installés. La visualisation ne sera pas disponible.\")\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Chemin vers le modèle Word2Vec\n",
    "model_path = \"../../data/newspapers.model\"\n",
    "\n",
    "# Chargement du modèle Word2Vec\n",
    "try:\n",
    "    model = Word2Vec.load(model_path)\n",
    "    print(f\"Modèle Word2Vec chargé depuis '{model_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Modèle introuvable à l'emplacement '{model_path}'\")\n",
    "    exit()\n",
    "\n",
    "# Exploration du modèle\n",
    "print(\"\\n=== Exploration du Modèle Word2Vec ===\")\n",
    "print(f\"Vocabulaire : {len(model.wv.key_to_index)} mots\")\n",
    "print(f\"Dimensions des vecteurs : {model.wv.vector_size}\")\n",
    "\n",
    "# Mots les plus fréquents\n",
    "print(\"\\n--- Mots les plus fréquents ---\")\n",
    "most_frequent_words = list(model.wv.key_to_index.keys())[:20]\n",
    "pprint(most_frequent_words)\n",
    "\n",
    "# Exemples de similarité\n",
    "print(\"\\n--- Exemples de similarité entre mots ---\")\n",
    "similarity_examples = [\n",
    "    (\"guerre\", \"conflit\"),\n",
    "    (\"vietnam\", \"américain\"),\n",
    "    (\"étudiant\", \"manifestation\"),\n",
    "    (\"musique\", \"artiste\"),\n",
    "    (\"ministre\", \"gouvernement\"),\n",
    "    (\"voiture\", \"ford\"),\n",
    "    (\"appartement\", \"villa\"),\n",
    "    (\"travail\", \"emploi\"),\n",
    "    (\"décès\", \"famille\"),\n",
    "    (\"chaise\", \"astronomie\")\n",
    "]\n",
    "\n",
    "for word1, word2 in similarity_examples:\n",
    "    if word1 in model.wv and word2 in model.wv:\n",
    "        sim = model.wv.similarity(word1, word2)\n",
    "        print(f\"Similarité entre '{word1}' et '{word2}': {sim:.4f}\")\n",
    "    else:\n",
    "        print(f\"Mots absents du vocabulaire : '{word1}', '{word2}'\")\n",
    "\n",
    "# Mots les plus similaires\n",
    "print(\"\\n--- Mots les plus similaires ---\")\n",
    "most_similar_examples = [\"président\", \"lune\", \"festival\", \"crise\", \"femme\"]\n",
    "for target_word in most_similar_examples:\n",
    "    if target_word in model.wv:\n",
    "        print(f\"\\nMots similaires à '{target_word}':\")\n",
    "        pprint(model.wv.most_similar(target_word, topn=10))\n",
    "    else:\n",
    "        print(f\"Mot absent du vocabulaire : '{target_word}'\")\n",
    "\n",
    "# Analogies sémantiques\n",
    "print(\"\\n--- Exemples d'analogies ---\")\n",
    "analogy_examples = [\n",
    "    ([\"londres\", \"france\"], [\"paris\"], \"Londres est à France ce que Paris est à X\"),\n",
    "    ([\"reine\", \"homme\"], [\"femme\"], \"Reine est à Femme ce que Homme est à X\"),\n",
    "    ([\"guerre\", \"russe\"], [\"paix\"], \"Guerre est à Paix ce que Russe est à X\"),\n",
    "    ([\"chanteur\", \"chanson\"], [\"acteur\"], \"Chanteur est à Chanson ce que Acteur est à X\")\n",
    "]\n",
    "\n",
    "for positive_words, negative_words, description in analogy_examples:\n",
    "    if all(word in model.wv for word in positive_words + negative_words):\n",
    "        print(f\"\\nAnalogie : {description}\")\n",
    "        pprint(model.wv.most_similar(positive=positive_words, negative=negative_words, topn=5))\n",
    "    else:\n",
    "        missing = [w for w in positive_words + negative_words if w not in model.wv]\n",
    "        print(f\"\\nAnalogie '{description}' : mots manquants : {missing}\")\n",
    "\n",
    "# Visualisation avec t-SNE\n",
    "if 'TSNE' in locals() and 'plt' in locals():\n",
    "    print(\"\\n--- Visualisation t-SNE ---\")\n",
    "    frequent_words_subset = list(model.wv.key_to_index.keys())[20:120]\n",
    "    specific_words = [\n",
    "        \"guerre\", \"paix\", \"vietnam\", \"union\", \"soviétique\", \"américain\", \"président\", \"politique\",\n",
    "        \"démocratie\", \"révolution\", \"étudiant\", \"manifestation\", \"jeunesse\", \"liberté\",\n",
    "        \"droit\", \"femme\", \"intégration\", \"raciale\", \"mur_de_berlin\", \"crise\",\n",
    "        \"lune\", \"espace\", \"cosmonaute\", \"satellite\", \"astronaute\", \"mission\",\n",
    "        \"musique\", \"film\", \"théâtre\", \"festival\", \"artiste\", \"chanteur\", \"rock\", \"pop\",\n",
    "        \"marché\", \"actions\", \"bourse\", \"économie\", \"francs\", \"dollar\", \"entreprise\",\n",
    "        \"paris\", \"londres\", \"bruxelles\", \"washington\", \"moscou\", \"berlin\", \"rome\", \"saigon\",\n",
    "        \"appartement\", \"villa\", \"louer\", \"vendre\", \"voiture\", \"ford\", \"peugeot\", \"emploi\", \"travail\", \"agence\", \"décès\"\n",
    "    ]\n",
    "\n",
    "    words_to_visualize = [w for w in set(frequent_words_subset + specific_words) if w in model.wv]\n",
    "\n",
    "    if len(words_to_visualize) > 50:\n",
    "        word_vectors = np.array([model.wv[word] for word in words_to_visualize])\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=2500, init='pca', learning_rate='auto')\n",
    "\n",
    "        vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for i, word in enumerate(words_to_visualize):\n",
    "            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1], s=10)\n",
    "            plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(3, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', fontsize=7)\n",
    "        plt.title(f\"Visualisation t-SNE des embeddings de {len(words_to_visualize)} mots\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"tsne_word_embeddings.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Image sauvegardée : tsne_word_embeddings.png\")\n",
    "    else:\n",
    "        print(\"Pas assez de mots pour une visualisation t-SNE significative.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
